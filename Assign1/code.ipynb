{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HathawayQAQ/COMP551-Machine-Learning/blob/main/Assignment1/code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLlL28Ne-Snh"
      },
      "source": [
        "## Import Statement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TffOYnJa9Wjc",
        "outputId": "b69553ed-96c3-419d-9191-6a0b8cef7737"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ucimlrepo in /Users/yonganyu/.local/share/virtualenvs/NLP_study-XcuxisTq/lib/python3.11/site-packages (0.0.7)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /Users/yonganyu/.local/share/virtualenvs/NLP_study-XcuxisTq/lib/python3.11/site-packages (from ucimlrepo) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2020.12.5 in /Users/yonganyu/.local/share/virtualenvs/NLP_study-XcuxisTq/lib/python3.11/site-packages (from ucimlrepo) (2024.2.2)\n",
            "Requirement already satisfied: numpy<2,>=1.23.2 in /Users/yonganyu/.local/share/virtualenvs/NLP_study-XcuxisTq/lib/python3.11/site-packages (from pandas>=1.0.0->ucimlrepo) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/yonganyu/.local/share/virtualenvs/NLP_study-XcuxisTq/lib/python3.11/site-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/yonganyu/.local/share/virtualenvs/NLP_study-XcuxisTq/lib/python3.11/site-packages (from pandas>=1.0.0->ucimlrepo) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/yonganyu/.local/share/virtualenvs/NLP_study-XcuxisTq/lib/python3.11/site-packages (from pandas>=1.0.0->ucimlrepo) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /Users/yonganyu/.local/share/virtualenvs/NLP_study-XcuxisTq/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "!pip install ucimlrepo\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0KxS7sd-W8z"
      },
      "source": [
        "# Task1: Acquire, preprocess, and analyze the data\n",
        "## Data Preperation\n",
        "1. Dataset1: Infrared Thermography Temperature (regression): [link](https://archive.ics.uci.edu/dataset/925/infrared+thermography+temperature+dataset)\n",
        "2. Dataset 2: CDC Diabetes Health Indicators (classification): [link](https://archive.ics.uci.edu/dataset/891/cdc+diabetes+health+indicators)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rkqf8vDnBL05"
      },
      "source": [
        "## Data acquisition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hllYmUX3-yJu"
      },
      "outputs": [],
      "source": [
        "infrared_thermography_temperature = fetch_ucirepo(id=925)\n",
        "X = infrared_thermography_temperature.data.features\n",
        "y = infrared_thermography_temperature.data.targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYFifiVgAzLk"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "w1VNh5In_lFP"
      },
      "outputs": [],
      "source": [
        "# Handle missing values\n",
        "nan_rows = X.isnull().any(axis=1)\n",
        "X_clean = X[~nan_rows]\n",
        "y_clean = y[~nan_rows]\n",
        "\n",
        "# Handle categorical features\n",
        "categorical_columns = ['Age', 'Gender', 'Ethnicity']\n",
        "X_dummies = pd.get_dummies(X_clean, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "# Convert boolean columns to integer\n",
        "bool_columns = X_dummies.select_dtypes(include=['bool']).columns\n",
        "for col in bool_columns:\n",
        "    X_dummies[col] = X_dummies[col].astype(int)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_dummies)\n",
        "\n",
        "# Select the target variable\n",
        "y_final = y_clean['aveOralM']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_final, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j25fBjIsF4hD"
      },
      "source": [
        "# Task2: Implement the Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhB-ZEXJF6la"
      },
      "source": [
        "## 1. Linear Regression (Analytical Solution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aXd0ofrS_ZZr"
      },
      "outputs": [],
      "source": [
        "class LinearRegressionModel:\n",
        "    def __init__(self):\n",
        "        self.weights = None\n",
        "\n",
        "    def fit(self, features, target):\n",
        "        num_samples, num_features = features.shape\n",
        "        features_with_bias = np.column_stack([np.ones(num_samples), features])  # Added bias term (ones)\n",
        "        self.weights, loss = self.calculate_least_squares_loss(features_with_bias, target)\n",
        "        return (loss, self.weights)\n",
        "\n",
        "    def calculate_least_squares_loss(self, features, target):\n",
        "        weights = np.linalg.inv(features.T @ features) @ features.T @ target\n",
        "        loss = np.mean((target - features @ weights) ** 2)\n",
        "        return weights, loss\n",
        "\n",
        "    def predict(self, features):\n",
        "        features_with_bias = np.column_stack([np.ones(features.shape[0]), features])  # Added bias term (ones)\n",
        "        return features_with_bias @ self.weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wIexX-gpHlLu"
      },
      "outputs": [],
      "source": [
        "class LinearRegressionSGD:\n",
        "  def __init__(self):\n",
        "        self.weights = None\n",
        "  def fit(self, features, target, max_iterations=1000, tolerance=1e-5, learning_rate=1e-2, verbose=False, batch_size=16):\n",
        "        num_samples, num_features = features.shape\n",
        "        features_with_bias = np.column_stack([np.ones(num_samples), features])\n",
        "        target = target.astype(int)\n",
        "        rng = np.random.default_rng()\n",
        "\n",
        "        if self.weights is None:\n",
        "            self.weights = 0.001 * rng.standard_normal(num_features + 1)  # Random initialization\n",
        "\n",
        "        loss_history = []\n",
        "        for iteration in range(max_iterations):\n",
        "            batch_indices = rng.choice(np.arange(num_samples), batch_size, replace=False)\n",
        "            features_batch = features_with_bias[batch_indices, :]\n",
        "            target_batch = target[batch_indices]\n",
        "\n",
        "            gradient, loss = self.linear_loss(features_batch, target_batch)\n",
        "            loss_history.append(loss)\n",
        "\n",
        "            self.weights -= learning_rate * gradient  # Update weights\n",
        "\n",
        "            if verbose and iteration % 100 == 0:\n",
        "                print(f\"Iteration {iteration}: Loss {loss}\")\n",
        "\n",
        "        return loss_history\n",
        "\n",
        "  def predict(self, features):\n",
        "      features_with_bias = np.column_stack([np.ones(features.shape[0]), features])\n",
        "      return features_with_bias @ self.weights\n",
        "\n",
        "  def linear_loss(self, features, target):\n",
        "      num_samples = features.shape[0]\n",
        "      gradient = np.dot(features.T, np.dot(features, self.weights) - target) / num_samples\n",
        "      loss = np.sum((target - features @ self.weights) ** 2) / num_samples\n",
        "      return gradient, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mtuym_9RIQz-"
      },
      "source": [
        "# Task3: Run Experiments\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etqK7v9WISe5"
      },
      "source": [
        "## Experiment 1: Report performance of linear regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VdXGs5NJIWnR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment 1: Linear Regression Performance\n",
            "Training MSE: 0.0617\n",
            "Test MSE: 0.0667\n",
            "Training R-squared: 0.7756\n",
            "Test R-squared: 0.6646\n",
            "MAE (Train): 0.1950\n",
            "MAE (Test): 0.2032\n"
          ]
        }
      ],
      "source": [
        "model = LinearRegressionModel()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = model.predict(X_train)\n",
        "y_test_pred = model.predict(X_test)\n",
        "\n",
        "train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "train_r2 = r2_score(y_train, y_train_pred)\n",
        "test_r2 = r2_score(y_test, y_test_pred)\n",
        "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
        "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
        "\n",
        "print(\"Experiment 1: Linear Regression Performance\")\n",
        "print(f\"Training MSE: {train_mse:.4f}\")\n",
        "print(f\"Test MSE: {test_mse:.4f}\")\n",
        "print(f\"Training R-squared: {train_r2:.4f}\")\n",
        "print(f\"Test R-squared: {test_r2:.4f}\")\n",
        "print(f\"MAE (Train): {mae_train:.4f}\")\n",
        "print(f\"MAE (Test): {mae_test:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3MXBM2mIYup"
      },
      "source": [
        "## Experiment 2: Report weights of features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zYJ2OiWhIcQl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 10 features (Linear Regression):\n",
            "        Feature    Weight  Abs_Weight\n",
            "19  canthi4Max1  0.357622    0.357622\n",
            "18   canthiMax1 -0.352782    0.352782\n",
            "27       T_Max1  0.301124    0.301124\n",
            "12        T_LC1  0.300552    0.300552\n",
            "11    T_RC_Max1  0.230246    0.230246\n",
            "28        T_OR1  0.212044    0.212044\n",
            "4     Max1R13_1 -0.209065    0.209065\n",
            "8         T_RC1 -0.159100    0.159100\n",
            "29    T_OR_Max1 -0.157372    0.157372\n",
            "9     T_RC_Dry1  0.143177    0.143177\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'[666, 30, 783] not in index'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(report_top_features(lr_model, feature_names))\n\u001b[1;32m     12\u001b[0m sgd_model \u001b[38;5;241m=\u001b[39m LinearRegressionSGD()\n\u001b[0;32m---> 13\u001b[0m \u001b[43msgd_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTop 10 features (SGD Linear Regression):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(report_top_features(sgd_model, feature_names))\n",
            "Cell \u001b[0;32mIn[6], line 17\u001b[0m, in \u001b[0;36mLinearRegressionSGD.fit\u001b[0;34m(self, features, target, max_iterations, tolerance, learning_rate, verbose, batch_size)\u001b[0m\n\u001b[1;32m     15\u001b[0m batch_indices \u001b[38;5;241m=\u001b[39m rng\u001b[38;5;241m.\u001b[39mchoice(np\u001b[38;5;241m.\u001b[39marange(num_samples), batch_size, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m features_batch \u001b[38;5;241m=\u001b[39m features_with_bias[batch_indices, :]\n\u001b[0;32m---> 17\u001b[0m target_batch \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_indices\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     19\u001b[0m gradient, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_loss(features_batch, target_batch)\n\u001b[1;32m     20\u001b[0m loss_history\u001b[38;5;241m.\u001b[39mappend(loss)\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/NLP_study-XcuxisTq/lib/python3.11/site-packages/pandas/core/series.py:1144\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1141\u001b[0m     key \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(key, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m   1142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_rows_with_mask(key)\n\u001b[0;32m-> 1144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_with\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/NLP_study-XcuxisTq/lib/python3.11/site-packages/pandas/core/series.py:1171\u001b[0m, in \u001b[0;36mSeries._get_with\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minteger\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;66;03m# We need to decide whether to treat this as a positional indexer\u001b[39;00m\n\u001b[1;32m   1169\u001b[0m     \u001b[38;5;66;03m#  (i.e. self.iloc) or label-based (i.e. self.loc)\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_should_fallback_to_positional:\n\u001b[0;32m-> 1171\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1172\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1173\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1174\u001b[0m             \u001b[38;5;66;03m# GH#50617\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeries.__getitem__ treating keys as positions is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1180\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m   1181\u001b[0m         )\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/NLP_study-XcuxisTq/lib/python3.11/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/NLP_study-XcuxisTq/lib/python3.11/site-packages/pandas/core/indexing.py:1420\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1418\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[1;32m   1423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/NLP_study-XcuxisTq/lib/python3.11/site-packages/pandas/core/indexing.py:1360\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[0;32m-> 1360\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[1;32m   1362\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1363\u001b[0m )\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/NLP_study-XcuxisTq/lib/python3.11/site-packages/pandas/core/indexing.py:1558\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1555\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[1;32m   1556\u001b[0m axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[0;32m-> 1558\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/NLP_study-XcuxisTq/lib/python3.11/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/NLP_study-XcuxisTq/lib/python3.11/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mKeyError\u001b[0m: '[666, 30, 783] not in index'"
          ]
        }
      ],
      "source": [
        "def report_top_features(model, feature_names, top_n=10):\n",
        "    weights = model.weights[1:]  # Exclude bias term\n",
        "    feature_importance = pd.DataFrame({'Feature': feature_names, 'Weight': weights})\n",
        "    feature_importance['Abs_Weight'] = abs(feature_importance['Weight'])\n",
        "    return feature_importance.sort_values('Abs_Weight', ascending=False).head(top_n)\n",
        "\n",
        "feature_names = X_dummies.columns.tolist()\n",
        "lr_model = LinearRegressionModel()\n",
        "lr_model.fit(X_train, y_train)\n",
        "print(\"\\nTop 10 features (Linear Regression):\")\n",
        "print(report_top_features(lr_model, feature_names))\n",
        "sgd_model = LinearRegressionSGD()\n",
        "sgd_model.fit(X_train, y_train, verbose=True)\n",
        "print(\"\\nTop 10 features (SGD Linear Regression):\")\n",
        "print(report_top_features(sgd_model, feature_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IK4LUW6IiCQ"
      },
      "source": [
        "## Experiment 3: Sample growing subsets of training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0UiSFoO5Ikd8"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'LinearRegression' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m size \u001b[38;5;129;01min\u001b[39;00m train_sizes:\n\u001b[1;32m      6\u001b[0m     x_train_subset, _, y_train_subset, _ \u001b[38;5;241m=\u001b[39m train_test_split(X_train, y_train, train_size\u001b[38;5;241m=\u001b[39msize, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mLinearRegression\u001b[49m()\n\u001b[1;32m      8\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(x_train_subset, y_train_subset)\n\u001b[1;32m     10\u001b[0m     train_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_train_subset)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'LinearRegression' is not defined"
          ]
        }
      ],
      "source": [
        "train_sizes = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "train_scores = []\n",
        "test_scores = []\n",
        "\n",
        "for size in train_sizes:\n",
        "    x_train_subset, _, y_train_subset, _ = train_test_split(X_train, y_train, train_size=size, random_state=42)\n",
        "    model = LinearRegression()\n",
        "    model.fit(x_train_subset, y_train_subset)\n",
        "\n",
        "    train_pred = model.predict(x_train_subset)\n",
        "    test_pred = model.predict(X_test)\n",
        "\n",
        "    train_scores.append(r2_score(y_train_subset, train_pred))\n",
        "    test_scores.append(r2_score(y_test, test_pred))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_sizes, train_scores, label='Train R-squared')\n",
        "plt.plot(train_sizes, test_scores, label='Test R-squared')\n",
        "plt.xlabel('Training Set Size')\n",
        "plt.ylabel('R-squared Score')\n",
        "plt.title('Learning Curve')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL7rtc1rIrbu"
      },
      "source": [
        "## Experiment 4: Try different mini-batch sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okTStqUgIuC9"
      },
      "outputs": [],
      "source": [
        "batch_sizes = [8, 16, 32, 64, 128]\n",
        "batch_scores = []\n",
        "\n",
        "def safe_r2_score(y_true, y_pred):\n",
        "    try:\n",
        "        return r2_score(y_true, y_pred)\n",
        "    except ValueError:\n",
        "        return float('-inf')\n",
        "\n",
        "for batch_size in batch_sizes:\n",
        "    model = SGDLinearRegression(batch_size=batch_size)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    score = safe_r2_score(y_test, y_pred)\n",
        "    batch_scores.append(score)\n",
        "    print(f\"Batch size {batch_size}: Test R2= {score:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(batch_sizes, batch_scores, marker='o')\n",
        "plt.xlabel('Batch Size')\n",
        "plt.ylabel('Test R2 Score')\n",
        "plt.title('Effect of Batch Size on Model Performance')\n",
        "plt.xscale('log')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nExperiment 4: Mini-batch Size Performance\")\n",
        "for batch_size, score in zip(batch_sizes, batch_scores):\n",
        "    print(f\"Batch size {batch_size}: Test R2 = {score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWHGCxb3IyTC"
      },
      "source": [
        "## Experiment 5: Try different learning rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8_fUTo6IzOS"
      },
      "outputs": [],
      "source": [
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "lr_scores = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    model = SGDLinearRegression(learning_rate=lr)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    score = safe_r2_score(y_test, y_pred)\n",
        "    lr_scores.append(score)\n",
        "    print(f\"Learning rate {lr}: Test R-squared = {score:.4f}\")\n",
        "\n",
        "print(\"\\nExperiment 5: Learning Rate Performance\")\n",
        "for lr, score in zip(learning_rates, lr_scores):\n",
        "    print(f\"Learning rate {lr}: Test R-squared = {score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jdmQXfRI2qR"
      },
      "source": [
        "## Experiment 6: Compare analytical solution with mini-batch SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqiXySk6I3ax"
      },
      "outputs": [],
      "source": [
        "analytical_model = LinearRegression()\n",
        "analytical_model.fit(X_train, y_train)\n",
        "analytical_pred = analytical_model.predict(X_test)\n",
        "analytical_score = r2_score(y_test, analytical_pred)\n",
        "\n",
        "sgd_model = SGDLinearRegression()\n",
        "sgd_model.fit(X_train, y_train)\n",
        "sgd_pred = sgd_model.predict(X_test)\n",
        "sgd_score = r2_score(y_test, sgd_pred)\n",
        "\n",
        "print(\"\\nExperiment 6: Analytical vs SGD Performance\")\n",
        "print(f\"Analytical solution: Test R-squared = {analytical_score:.4f}\")\n",
        "print(f\"Mini-batch SGD: Test R-squared = {sgd_score:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "NLP_study-XcuxisTq",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
